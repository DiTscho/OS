{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set path to directory containing images\n",
    "path_imgs= Path(\"../wetransfer-ce6b2b/all_images\")\n",
    "\n",
    "# Extract image names and labels from directory\n",
    "img_names= ! ls ../wetransfer-ce6b2b/all_images/ \n",
    "img_labels= [x[7] for x in img_names]\n",
    "\n",
    "# Create a pandas dataframe containing filenames and corresponding labels\n",
    "labels_df= pd.DataFrame(list(zip(img_names, img_labels)), columns=[\"filenames\", \"labels\"])\n",
    "\n",
    "# Remove any \"c\" labels from the dataframe and convert labels to int8 datatype\n",
    "labels_df = labels_df[labels_df[\"labels\"]!=\"c\"]\n",
    "labels_df[\"labels\"] = labels_df.labels.astype(np.int8)\n",
    "\n",
    "# Define a custom image dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame with filenames in the first column and labels in the second column.\n",
    "            root_dir (str): Directory containing the images.\n",
    "            transform (callable, optional): Optional transform to apply to images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        idx = int(idx)  # Convert the index to an integer\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.dataframe.iloc[idx, 1]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Set the directory containing the images\n",
    "root_dir = str(path_imgs)\n",
    "\n",
    "# Define any image transformations you want to apply\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "dataset = CustomImageDataset(dataframe=labels_df, root_dir=root_dir, transform=transform)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_ratio = 0.8  # Set the ratio of the training dataset\n",
    "total_samples = len(dataset)\n",
    "train_size = int(train_ratio * total_samples)\n",
    "test_size = total_samples - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Set batch size and number of workers for dataloader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "\n",
    "# Create data loaders for training and testing sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "class ResNet18WithDropout(nn.Module):\n",
    "    def __init__(self, resnet18_model, dropout_prob=0.5):\n",
    "        super(ResNet18WithDropout, self).__init__()\n",
    "        # Initialize the resnet18 model and dropout layer\n",
    "        self.resnet18 = resnet18_model\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        # Replace the last fully connected layer with our own\n",
    "        self.fc = nn.Linear(resnet18_model.fc.in_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the resnet18 model\n",
    "        x = self.resnet18.conv1(x)\n",
    "        x = self.resnet18.bn1(x)\n",
    "        x = self.resnet18.relu(x)\n",
    "        x = self.resnet18.maxpool(x)\n",
    "        x = self.resnet18.layer1(x)\n",
    "        x = self.resnet18.layer2(x)\n",
    "        x = self.resnet18.layer3(x)\n",
    "        x = self.resnet18.layer4(x)\n",
    "        x = self.resnet18.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Apply dropout and pass through the fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the ResNet18 model with dropout and replace the last fully connected layer with our own\n",
    "resnet18_with_dropout = ResNet18WithDropout(resnet18, dropout_prob=0.5)\n",
    "resnet18_with_dropout.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Move the model and data to the device (GPU or CPU) for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet18_with_dropout = resnet18_with_dropout.to(device)\n",
    "resnet18 = resnet18.to(device)\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        # Move the data to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients and forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss and perform backpropagation\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss and accuracy\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    # Compute the epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize loss and correct predictions to 0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Disable gradient calculation during validation\n",
    "    with torch.no_grad():\n",
    "        # Loop through batches in dataloader\n",
    "        for images, labels in dataloader:\n",
    "            # Send images and labels to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Compute model outputs\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update loss and correct predictions\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Compute average loss and accuracy over the dataset\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "    # Return average loss and accuracy\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_losses, test_losses):\n",
    "    # Plot train and test loss curves\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize lists to store train and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop through epochs\n",
    "for epoch in range(0,num_epochs):\n",
    "    # Train the model and compute train and validation losses\n",
    "    train_loss, train_acc = train(resnet18_with_dropout, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(resnet18_with_dropout, test_loader, criterion, device)\n",
    "\n",
    "    # Update the learning rate scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Append train and validation losses to respective lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print epoch results\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {train_loss:.4f},\\\n",
    "          Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
